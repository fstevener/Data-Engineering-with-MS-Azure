{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "546beb5c-b6dc-443b-851a-48cebd156403",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Project 3: Building an Azure Data Lake for Bike Share Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c38c042-cdde-4761-993c-8062c677019b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extract Step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc433cde-8991-4913-8d65-c01966fedb43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write payments.csv from FileStore to the Delta File System\n",
    "\n",
    "df_payments = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"/FileStore/tables/payments.csv\")\n",
    "\n",
    "\n",
    "df_payments.write.format(\"delta\").save(\"/delta/bronze_payments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562cd6a2-63b5-4c07-b498-dfce48af16ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write riders.csv from FileStore to the Delta File System\n",
    "\n",
    "df_riders = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"/FileStore/tables/riders.csv\")\n",
    "\n",
    "\n",
    "df_riders.write.format(\"delta\").save(\"/delta/bronze_riders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d75b3f9-9564-4135-9ae8-15b1ba2d923b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write stations.csv from FileStore to the Delta File System\n",
    "\n",
    "df_stations = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"/FileStore/tables/stations.csv\")\n",
    "\n",
    "\n",
    "df_stations.write.format(\"delta\").save(\"/delta/bronze_stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c67aef6-81d2-4992-b48f-2a6d983da3a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write trips.csv from FileStore to the Delta File System\n",
    "\n",
    "df_trips = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"/FileStore/tables/trips.csv\")\n",
    "\n",
    "\n",
    "df_trips.write.format(\"delta\").save(\"/delta/bronze_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e16dc6-2e9c-46a9-b417-33b8294ee9f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write dimdates.csv from FileStore to the Delta File System\n",
    "\n",
    "df_dimdates = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .load(\"/FileStore/tables/dimdates.csv\")\n",
    "\n",
    "\n",
    "df_dimdates.write.format(\"delta\").save(\"/delta/bronze_dimdates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c269b12c-eabb-4ec6-8266-2fa21023390b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c233b795-3125-4260-854f-a54647214284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create delta table for the payments data\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE bronze_payments_deltaTable\n",
    "    USING DELTA\n",
    "    LOCATION '/delta/bronze_payments'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6522cef8-1714-4366-a013-60f7af9b18d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create delta table for riders data\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE bronze_riders_deltaTable\n",
    "    USING DELTA\n",
    "    LOCATION '/delta/bronze_riders'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d572299-2d3f-4c82-875f-0f77257a205f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create delta table for stations data\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE bronze_stations_deltaTable\n",
    "    USING DELTA\n",
    "    LOCATION '/delta/bronze_stations'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c3de40-687f-4c61-a3f5-56cdac0ae6a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create delta table for trips data\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE bronze_trips_deltaTable\n",
    "    USING DELTA\n",
    "    LOCATION '/delta/bronze_trips'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c32789-ef8b-47ac-8ce5-20dee1e3f07c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create delta table for date dimension\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE bronze_dimdate_deltaTable\n",
    "    USING DELTA\n",
    "    LOCATION '/delta/bronze_dimdates'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89af5780-7610-4fe0-be3f-fe44ca855751",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e189c5-d62d-4348-9579-c6eb8fbbd8e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create golden delta table for date dimension\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "CREATE TABLE gold_dimDate_deltaTable AS\n",
    "SELECT \n",
    "CAST( DateDimId AS BIGINT) AS DateDimId,\n",
    "CAST( DateActual AS DATE) AS DateActual,\n",
    "CAST( DateLongDescription AS STRING) AS DateLongDescription,\n",
    "CAST( DayLongName AS STRING) AS DayLongName,\n",
    "CAST( MonthLongName AS STRING) AS MonthLongName,\n",
    "CAST( CalendarDay AS BIGINT) AS CalendarDay,\n",
    "CAST( CalendarWeek AS BIGINT) AS CalendarWeek,\n",
    "CAST( CalendarDayInWeek AS BIGINT) AS CalendarDayInWeek,\n",
    "CAST( CalendarMonth AS BIGINT) AS CalendarMonth,\n",
    "CAST( CalendarQuarter AS BIGINT) AS CalendarQuarter,\n",
    "CAST( CalendarYear AS BIGINT) AS CalendarYear,\n",
    "CAST( FiscalDayInWeek AS BIGINT) AS FiscalDayInWeek,\n",
    "CAST( FiscalMonth AS BIGINT) AS FiscalMonth,\n",
    "CAST( FiscalDayInMonth AS BIGINT) AS FiscalDayInMonth,\n",
    "CAST( FiscalQuarter AS BIGINT) AS FiscalQuarter,\n",
    "CAST( FiscalYear AS BIGINT) AS FiscalYear\n",
    "FROM bronze_dimdate_deltaTable;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1755fb6c-51a6-49b3-b3fd-15c4e0ada675",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create golden delta table for rider dimension\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "CREATE TABLE gold_dimRiders_deltaTable AS\n",
    "SELECT \n",
    "CAST( _c0 AS BIGINT) AS RiderId,\n",
    "CAST( _c1 AS STRING) AS First,\n",
    "CAST( _c2 AS STRING) AS Last,\n",
    "CAST( _c3 AS STRING) AS Address,\n",
    "CAST( _c4 AS DATE) AS Birthday,\n",
    "CAST( _c5 AS DATE) AS AccountStartDate,\n",
    "CAST( _c6 AS DATE) AS AccountEndDate,\n",
    "CAST( _c7 AS BOOLEAN) AS Member\n",
    "FROM bronze_riders_deltaTable;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "241832f1-8199-49d2-a1ee-f4e84b163cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create golden delta table for station dimension\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "CREATE TABLE gold_dimStations_deltaTable AS\n",
    "SELECT \n",
    "CAST( _c0 AS STRING) AS StationId,\n",
    "CAST( _c1 AS STRING) AS Name,\n",
    "CAST( _c2 AS FLOAT) AS Latitude,\n",
    "CAST( _c3 AS FLOAT) AS Longitude\n",
    "FROM bronze_stations_deltaTable;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2c4d79-dcb1-4515-89f3-a02e01a4a6b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create golden delta table for payments fact\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "CREATE TABLE gold_factPayments_deltaTable AS\n",
    "SELECT \n",
    "CAST( T1._c0 AS BIGINT) AS PaymentId,\n",
    "CAST( T2.DateDimId AS BIGINT) AS DateDimId,\n",
    "CAST( T1._c3 AS BIGINT) AS RiderId,\n",
    "CAST( T1._c1 AS DATE) AS PaymentDate,\n",
    "CAST( T1._c2 AS FLOAT) AS Amount\n",
    "FROM bronze_payments_deltaTable AS T1\n",
    "INNER JOIN gold_dimdate_deltatable AS T2\n",
    "ON T2.DateActual = CAST( T1._c1 AS DATE);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78958382-b3d0-47bd-a5d1-85cc941083a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create golden delta table for trips fact\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "CREATE TABLE gold_factTrips_deltaTable AS\n",
    "SELECT \n",
    "CAST( T1._c0 AS STRING) AS TripId,\n",
    "CAST( T2.DateDimId AS BIGINT) AS DateDimId,\n",
    "CAST( T1._c6 AS BIGINT) AS RiderId,\n",
    "CAST( T1._c1 AS STRING) AS RideableType,\n",
    "CAST( T1._c4 AS STRING) AS StartStationId,\n",
    "CAST( T1._c5 AS STRING) AS EndStationId,\n",
    "CAST( T1._c2 AS TIMESTAMP) AS StartedAt,\n",
    "CAST( T1._c3 AS TIMESTAMP) AS EndedAt,\n",
    "DATEDIFF(MINUTE, CAST( T1._c2 AS TIMESTAMP), CAST( T1._c3 AS TIMESTAMP)) AS TripDurationInMinutes,\n",
    "DATEDIFF(YEAR, T3.Birthday, CAST( T1._c2 AS DATE)) AS RiderAge\n",
    "FROM bronze_trips_deltaTable AS T1\n",
    "INNER JOIN gold_dimdate_deltatable AS T2\n",
    "    ON T2.DateActual = CAST( T1._c2 AS DATE)\n",
    "INNER JOIN gold_dimriders_deltatable AS T3\n",
    "    ON T3.RiderId = CAST( T1._c6 AS BIGINT);\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkCode_project3 (2)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
